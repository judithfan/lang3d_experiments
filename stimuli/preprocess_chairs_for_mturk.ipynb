{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "##from skimage import data, io, filters\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot,pylab\n",
    "plt = pyplot\n",
    "import scipy\n",
    "from __future__ import division\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JEF notes 8/20/17: Apply this processing for running the chairs140 experiment. All 140 triplet families are included, and only the close condition triplets were added to the database. This can be modified in the \"upload stim dictionary to mongo\" codeblock.\n",
    "\n",
    "JEF notes 9/18/17: Adapting this preprocessing for maxdeg1138 dataset. All triplet families are uploaded and added to database. These include \"close\" and \"far\" triplets.\n",
    "\n",
    "JEF notes 3/27/18: Prepping for wave3 of data collection to \"patch\" missing utterances on existing chairs1k exemplars.\n",
    "\n",
    "JEF notes 4/12/18: Prepping for wave4 of data collection to \"expand\" to chairs2k dataset. Goal is to get at least 4 utterances for each of the new triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy and rename chairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphanum = dict(zip(range(26),string.ascii_lowercase))\n",
    "conds = ['close','far']\n",
    "upload_dir = './to_upload_chairs2k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## path to images\n",
    "## old_path_to_images = './geometry/chairs_1K_v1/entire_collection_max_deg_sampling' \n",
    "path_to_images = './chairs2k'\n",
    "path_to_close = os.path.join(path_to_images,'closest_nn','data')\n",
    "path_to_far = os.path.join(path_to_images,'far_enough','data')\n",
    "all_paths = [path_to_close,path_to_far]\n",
    "\n",
    "## path -- get list of all triplets\n",
    "valid = [d for d in os.listdir(path_to_close) if os.path.isdir(os.path.join(path_to_close, d))]\n",
    "valid = map(int,valid)\n",
    "\n",
    "# read in the intersection of rxdh and jefan's heuristic filtering of triplets\n",
    "filtered = 1 # filtered=1 if only the best triplets are used, otherwise set to 0 to use all triplets\n",
    "if filtered == True:\n",
    "    _discard = pd.read_csv(os.path.join(path_to_images,'manually_discarded_triplets.txt'))\n",
    "    discard = list(np.squeeze(_discard.values))\n",
    "    valid = [i for i in valid if i not in discard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of discarded triplet families: 205\n",
      "Total number of valid triplet families: 2026\n"
     ]
    }
   ],
   "source": [
    "print 'Total number of discarded triplet families: {}'.format(len(discard))\n",
    "print 'Total number of valid triplet families: {}'.format(len(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### crop image to make sure it's square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "runThis = 0\n",
    "if runThis:\n",
    "    rw = 224 # final width of images\n",
    "    rh = 224   \n",
    "    for cc,cond in enumerate(all_paths):\n",
    "        for family in os.listdir(cond):\n",
    "            if (family != 'model_names_of_triplets.txt') & (family in map(str,valid)):\n",
    "                all_images = os.listdir(os.path.join(cond,family))\n",
    "                for i, pix in enumerate(all_images):\n",
    "                    img = Image.open(os.path.join(cond,family,pix))\n",
    "                    w = np.shape(img)[1] \n",
    "                    h = np.shape(img)[0]                     \n",
    "                    img = img.crop( (int(w/2-rw/2),int(h/2-rh/2),int(w/2+rw/2),int(h/2+rh/2)) )\n",
    "                    img.thumbnail([224, 224], Image.ANTIALIAS)                    \n",
    "                    new_filename = conds[cc] + '_' + family + '_' + alphanum[i] + '_' +  all_images[i].split('.')[0].split('_')[1] + '.png'\n",
    "                    if not os.path.exists(upload_dir):\n",
    "                        os.makedirs(upload_dir)\n",
    "                    new_filepath = os.path.join(upload_dir,new_filename)\n",
    "                    imagefile = open(new_filepath, 'wb')\n",
    "                    try:\n",
    "                        img.save(imagefile, \"png\", quality=90)\n",
    "                        imagefile.close()\n",
    "                    except:\n",
    "                        print \"Cannot save user image\"                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_img = upload_dir\n",
    "all_files = os.listdir(path_to_img)\n",
    "all_files = [i for i in all_files if i != '.DS_Store']\n",
    "num_conds = 2\n",
    "num_in_triplet = 3\n",
    "assert len(all_files) == len(valid)*num_conds*num_in_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import boto\n",
    "bucket_name = 'shapenet-chairs-triplets-2k' ## options: ['shapenet-chairs-triplets-2k','shapenet_chair_triplets_1k']\n",
    "runThis = 0\n",
    "if runThis:\n",
    "    conn = boto.connect_s3()\n",
    "    b = conn.get_bucket(bucket_name) ### if bucket already exists, then get_bucket, else create_bucket\n",
    "    path_to_img = upload_dir\n",
    "    all_files = os.listdir(path_to_img)\n",
    "    for a in all_files:\n",
    "        print a\n",
    "        if a != '.DS_Store':\n",
    "            k = b.new_key(a)\n",
    "            k.set_contents_from_filename(os.path.join(path_to_img,a))\n",
    "            k.set_acl('public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build stimulus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_img = upload_dir\n",
    "all_files = os.listdir(path_to_img)\n",
    "all_files = [i for i in all_files if i != '.DS_Store']\n",
    "num_conds = 2\n",
    "num_in_triplet = 3\n",
    "assert len(all_files) == len(valid)*num_conds*num_in_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write out to .js file\n",
    "text_file = open(\"/Users/judithfan/language_and_3d/mturk/experiments/stimList_chairs2k.js\",\"w\")\n",
    "for a in all_files:\n",
    "    tmp = a.split('_')\n",
    "    line = 'var '+ a.split('.')[0] + ' = {filename: \"' + a + '\", condition: \"' + tmp[0] + '\", family: \"' + tmp[1] + '\", member: \"' + tmp[2].split('.')[0] + '\", shapenet_id: \"' + tmp[3] + '\", url: \"https://s3.amazonaws.com/shapenet_chair_triplets_1k/' + a + '\"};\\n'\n",
    "    text_file.write(line)\n",
    "text_file.write('var objectList = [\\n')    \n",
    "for i,f in enumerate(all_files):\n",
    "    if i < len(all_files)-1:\n",
    "        text_file.write( f.split('.')[0] + ',')   \n",
    "    else:\n",
    "        text_file.write( f.split('.')[0])   \n",
    "text_file.write('];\\n')\n",
    "text_file.write('module.exports = objectList;')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### prep json to upload stim dictionary to mongo (db = 'stimuli', collection='chairs1k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## should each record be a triplet?\n",
    "## with each field being a list of id's\n",
    "## target_status_dict = dict(zip(['a','b','c'],['target','distractor1','distractor2']))\n",
    "def rotate(l, n):\n",
    "    return l[n:] + l[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract all prefixes (unique triplet families)\n",
    "print('Generating list of unique triplets...')\n",
    "_prefix = []\n",
    "for a in all_files:\n",
    "    _tmp = a.split('_')[0] + '_' + a.split('_')[1] \n",
    "    _prefix.append([i for i in all_files if i.split('_')[0]+'_'+i.split('_')[1]==_tmp])\n",
    "prefix = [list(x) for x in set(tuple(x) for x in _prefix)]\n",
    "assert len(prefix)==len(all_files)/3  \n",
    "assert np.unique([len(p) for p in prefix])[0]==3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Generating list of triplets and their attributes...')    \n",
    "# generate pandas dataframe with different attributes\n",
    "filename = []\n",
    "condition = []\n",
    "family = []\n",
    "member = []\n",
    "shapenet_id = []\n",
    "url = []\n",
    "games = [] # this field keeps track of which games this triplet has been shown in\n",
    "target_status = [] # whether this object is the target\n",
    "for v in range(3): # indexes the \"version\" -- cycles through each object being the target\n",
    "    target_status_dict = dict(zip(['a','b','c'],rotate(['target','distractor1','distractor2'],v))) # rotate which object is target each time    \n",
    "    for triplet in prefix:        \n",
    "        tmp = triplet[0].split('_')\n",
    "        filename.append(triplet)\n",
    "        condition.append(triplet[0].split('_')[0])\n",
    "        family.append(triplet[0].split('_')[1])\n",
    "        member.append([t.split('_')[2].split('.')[0] for t in triplet])\n",
    "        shapenet_id.append([t.split('_')[3].split('.')[0] for t in triplet])\n",
    "        url.append([\"https://s3.amazonaws.com/{}/\".format(bucket_name) + t for t in triplet])\n",
    "        games.append([])    \n",
    "        target_status.append(rotate(['target','distractor1','distractor2'],v))\n",
    "assert len(target_status)==len(prefix)*3        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Generating pandas dataframe...')    \n",
    "table = [filename,condition,family,member,shapenet_id,url,games,target_status]\n",
    "headers = ['filename','condition','family','member','shapenet_id','url','games','target_status']\n",
    "df = pd.DataFrame(table)\n",
    "df = df.transpose()\n",
    "df.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chairs1k_families = map(str,np.arange(1138))\n",
    "chairs2k_expansion_only = df[~df['family'].isin(chairs1k_families)] ## make sure this logic is right\n",
    "assert np.intersect1d(np.unique(chairs2k_expansion_only['family'].values),chairs1k_families).size == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## which dataframe to write out ## options: [df,chairs2k_expansion_only]\n",
    "out_df = chairs2k_expansion_only\n",
    "## write out dataset to json file\n",
    "stimdict = out_df.to_dict(orient='records')\n",
    "import json\n",
    "dataset_name = 'chairs2k_expansion_only' ## options: ['chairs1k','chairs2k','chairs2k_expansion_only'] \n",
    "with open('{}.js'.format(dataset_name), 'w') as fout:\n",
    "    json.dump(stimdict, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload json to mongo (db = 'stimuli', coll = 'chairs2k') April 12 2018¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JEF notes 4/12/18: Goal is to expand the dataset by getting at least 4 utterances for the new triplets from family 1138-2231. Otherwise exactly the same as previous waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### next todo is to upload this JSON to initialize the new stimulus collection\n",
    "import json\n",
    "J = json.loads(open('{}.js'.format(dataset_name),mode='ru').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##assert len(J)==len(all_files)\n",
    "print 'dataset_name: {}'.format(dataset_name)\n",
    "print len(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'rxdhawkins.me' ## cocolab ip address\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import pymongo as pm\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['stimuli']\n",
    "coll = db[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## actually add data now to the database\n",
    "for (i,j) in enumerate(J):\n",
    "    if i%500==0:\n",
    "        print ('%d of %d' % (i,len(J)))\n",
    "    coll.insert_one(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## For patching experiment only: upload json to mongo (db = 'stimuli', coll = 'chairs1k') April 1 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JEF notes 3/27/18: Goal is to \"patch\" the dataset by targeting data collection on triplets for which we are missing utterances so it is as uniform (counterbalanced) as possible. We had been marking the full batch of 70 triplets when retrieving them from the db, which ended up being inaccurate for sessions in which pairs might have dropped out early. So we're going to archive that stimulus collection and rename it \"chairs1k_archived.\" Then we're going to upload a \"clone\" of chairs1k dataset where the marking is consistent with the groupdata csv (so in other words, only triplets that actually have utterance data are marked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### next todo is to upload this JSON to initialize the new stimulus collection\n",
    "import json\n",
    "J = json.loads(open('chairs1k_annotated_032718.js',mode='ru').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##assert len(J)==len(all_files)\n",
    "print len(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'rxdhawkins.me' ## cocolab ip address\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import pymongo as pm\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['stimuli']\n",
    "coll = db['chairs1k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### if you need to reset the stim db back to the way it was before we needed to do \"patching\", \n",
    "#### only under those circumstances, drop the chairs1k collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## actually add data now to the database\n",
    "for (i,j) in enumerate(J):\n",
    "    if i%500==0:\n",
    "        print ('%d of %d' % (i,len(J)))\n",
    "    coll.insert_one(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coll.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = coll.find({'family':'1'}).sort('condition')\n",
    "for rec in a:\n",
    "    print rec['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
