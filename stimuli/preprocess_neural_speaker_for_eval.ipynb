{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "##from skimage import data, io, filters\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot,pylab\n",
    "plt = pyplot\n",
    "import scipy\n",
    "from __future__ import division\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import string\n",
    "import pandas as pd\n",
    "import json\n",
    "import pymongo as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper funcs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this helps to sort in human order\n",
    "import re\n",
    "\n",
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "     \n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "\n",
    "def sort_nicely(l):\n",
    "    \"\"\" Sort the given list in the way that humans expect.\n",
    "    \"\"\"\n",
    "    l.sort(key=alphanum_key)\n",
    "    \n",
    "def load_text(path):\n",
    "    with open(path, 'r') as f:\n",
    "        x = f.readlines()\n",
    "    utt = x[0]\n",
    "    # replace special tokens with question marks\n",
    "    if '<DIA>' in utt:\n",
    "        utt = utt.replace('<DIA>', '-')\n",
    "    if '<UKN>' in utt:\n",
    "        utt = utt.replace('<UKN>', '___')    \n",
    "    return utt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "alphanum = dict(zip(range(26),string.ascii_lowercase))\n",
    "conditions = ['literal','pragmatic']\n",
    "upload_dir = './context_agnostic_False_rs_33'\n",
    "bucket_name = 'shapenet-chairs-speaker-eval' \n",
    "dataset_name = 'shapenet_chairs_speaker_eval' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get list of triplet dirs\n",
    "triplet_dirs = [i for i in os.listdir(upload_dir) if i != '.DS_Store']\n",
    "triplet_dirs = [i for i in triplet_dirs if i[:7]=='triplet']\n",
    "triplet_dirs = [os.path.join(upload_dir,i) for i in triplet_dirs]\n",
    "sort_nicely(triplet_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go through and rename the images from 0,1,2 to distractor1,distractor2,target\n",
    "for this_triplet in triplet_dirs:\n",
    "    if os.path.exists(os.path.join(this_triplet,'0.png')):    \n",
    "        _shapenet_ids = np.load(os.path.join(this_triplet,'shape_net_ids.npy'))\n",
    "        shapenet_id_dict = dict(zip(['distractor1','distractor2','target'],_shapenet_ids))\n",
    "        os.rename(os.path.join(this_triplet,'0.png'),os.path.join(this_triplet,'{}_distractor1.png'.format(shapenet_id_dict['distractor1'])))\n",
    "        os.rename(os.path.join(this_triplet,'1.png'),os.path.join(this_triplet,'{}_distractor2.png'.format(shapenet_id_dict['distractor2'])))\n",
    "        os.rename(os.path.join(this_triplet,'2.png'),os.path.join(this_triplet,'{}_target.png'.format(shapenet_id_dict['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _shapenet_ids = np.load(os.path.join(this_triplet,'shape_net_ids.npy'))\n",
    "# shapenet_id_dict = dict(zip(['distractor1','distractor2','target'],_shapenet_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# literal_utt = load_text(os.path.join(this_triplet,'literal_utterance.txt'))\n",
    "# pragmatic_utt = load_text(os.path.join(this_triplet,'pragmatic_utterance.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload stims to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto\n",
    "runThis = 0\n",
    "if runThis:\n",
    "    conn = boto.connect_s3()\n",
    "    b = conn.create_bucket(bucket_name) ### if bucket already exists, then get_bucket, else create_bucket\n",
    "    for ind,this_triplet in enumerate(triplet_dirs):\n",
    "        ims = [i for i in os.listdir(this_triplet) if i[-3:]=='png']\n",
    "        for im in ims:\n",
    "            print ind, im\n",
    "            k = b.new_key(im)\n",
    "            k.set_contents_from_filename(os.path.join(this_triplet,im))\n",
    "            k.set_acl('public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build stimulus dictionary & upload metadata to mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list of triplets and their attributes...\n"
     ]
    }
   ],
   "source": [
    "print('Generating list of triplets and their attributes...')    \n",
    "# generate pandas dataframe with different attributes\n",
    "condition = []\n",
    "family = []\n",
    "utt = []\n",
    "target = []\n",
    "distractor1 = []\n",
    "distractor2 = []\n",
    "games = [] # this field keeps track of which games this triplet has been shown in\n",
    "shuffler_ind = []\n",
    "\n",
    "## generate permuted list of triplet indices in order to be able retrieve from triplets pseudorandomly\n",
    "inds = np.arange(len(conditions)*len(triplet_dirs))\n",
    "shuffled_inds = np.random.RandomState(0).permutation(inds)\n",
    "counter = 0\n",
    "for cond_ind,this_condition in enumerate(conditions):\n",
    "    for trip_ind,this_triplet in enumerate(triplet_dirs):        \n",
    "        ims = [i for i in os.listdir(this_triplet) if i[-3:]=='png']\n",
    "        # extract filename\n",
    "        target_filename = [i for i in ims if 'target' in i][0]\n",
    "        distractor1_filename = [i for i in ims if 'distractor1' in i][0]\n",
    "        distractor2_filename = [i for i in ims if 'distractor2' in i][0]\n",
    "        # define url\n",
    "        target_url = 'https://s3.amazonaws.com/{}/{}'.format(bucket_name,target_filename)\n",
    "        distractor1_url = 'https://s3.amazonaws.com/{}/{}'.format(bucket_name,distractor1_filename)        \n",
    "        distractor2_url = 'https://s3.amazonaws.com/{}/{}'.format(bucket_name,distractor2_filename)\n",
    "        # extract shapenetid\n",
    "        target_shapenetid = target_filename.split('_')[0]\n",
    "        distractor1_shapenetid = distractor1_filename.split('_')[0]\n",
    "        distractor2_shapenetid = distractor2_filename.split('_')[0]\n",
    "        # roll metadata into targ, d1, d2 dictionaries\n",
    "        _target = {'filename': target_filename, 'url': target_url, 'shapenetid': target_shapenetid}\n",
    "        _distractor1 = {'filename': distractor1_filename, 'url': distractor1_url, 'shapenetid': distractor1_shapenetid}\n",
    "        _distractor2 = {'filename': distractor2_filename, 'url': distractor2_url, 'shapenetid': distractor2_shapenetid}\n",
    "        # extract family and utt info\n",
    "        this_family = this_triplet.split('/')[-1]        \n",
    "        this_utt = load_text(os.path.join(this_triplet,'{}_utterance.txt'.format(this_condition)))        \n",
    "        # append to lists to prep for dataframe\n",
    "        condition.append(this_condition)\n",
    "        family.append(this_family)\n",
    "        utt.append(this_utt)\n",
    "        target.append(_target)\n",
    "        distractor1.append(_distractor1)\n",
    "        distractor2.append(_distractor2)\n",
    "        games.append([])\n",
    "        shuffler_ind.append(shuffled_inds[counter])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pandas dataframe...\n"
     ]
    }
   ],
   "source": [
    "print('Generating pandas dataframe...') \n",
    "table = [condition,family,utt,target,distractor1,distractor2,games,shuffler_ind]\n",
    "headers = ['condition','family','utt','target','distractor1','distractor2','games','shuffler_ind']\n",
    "df = pd.DataFrame(table)\n",
    "df = df.transpose()\n",
    "df.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving out json dictionary out to file...\n"
     ]
    }
   ],
   "source": [
    "## save out to file\n",
    "print('Saving out json dictionary out to file...') \n",
    "stimdict = df.to_dict(orient='records') \n",
    "with open('{}.js'.format(dataset_name), 'w') as fout:\n",
    "    json.dump(stimdict, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next todo is to upload this JSON to initialize the new stimulus collection...\n"
     ]
    }
   ],
   "source": [
    "### next todo is to upload this JSON to initialize the new stimulus collection\n",
    "print('next todo is to upload this JSON to initialize the new stimulus collection...')\n",
    "import json\n",
    "J = json.loads(open('{}.js'.format(dataset_name),mode='ru').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name: shapenet_chairs_speaker_eval\n",
      "2408\n"
     ]
    }
   ],
   "source": [
    "##assert len(J)==len(all_files)\n",
    "print 'dataset_name: {}'.format(dataset_name)\n",
    "print len(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'rxdhawkins.me' ## cocolab ip address\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['stimuli']\n",
    "coll = db[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 2408\n",
      "100 of 2408\n",
      "200 of 2408\n",
      "300 of 2408\n",
      "400 of 2408\n",
      "500 of 2408\n",
      "600 of 2408\n",
      "700 of 2408\n",
      "800 of 2408\n",
      "900 of 2408\n",
      "1000 of 2408\n",
      "1100 of 2408\n",
      "1200 of 2408\n",
      "1300 of 2408\n",
      "1400 of 2408\n",
      "1500 of 2408\n",
      "1600 of 2408\n",
      "1700 of 2408\n",
      "1800 of 2408\n",
      "1900 of 2408\n",
      "2000 of 2408\n",
      "2100 of 2408\n",
      "2200 of 2408\n",
      "2300 of 2408\n",
      "2400 of 2408\n"
     ]
    }
   ],
   "source": [
    "## actually add data now to the database\n",
    "reallyRun = 1\n",
    "if reallyRun:\n",
    "    for (i,j) in enumerate(J):\n",
    "        if i%100==0:\n",
    "            print ('%d of %d' % (i,len(J)))\n",
    "        coll.insert_one(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## check how many records have been retrieved\n",
    "a = coll.find({'shuffler_ind':{'$gte':0}})\n",
    "numGames = []\n",
    "for rec in a:\n",
    "    numGames.append(len(rec['games']))\n",
    "b = np.array(numGames)\n",
    "print np.mean(b>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
